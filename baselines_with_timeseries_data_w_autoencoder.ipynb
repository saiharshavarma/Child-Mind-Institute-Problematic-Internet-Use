{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:30:28.980315Z",
     "iopub.status.busy": "2024-11-27T20:30:28.979862Z",
     "iopub.status.idle": "2024-11-27T20:30:28.987176Z",
     "shell.execute_reply": "2024-11-27T20:30:28.986243Z",
     "shell.execute_reply.started": "2024-11-27T20:30:28.980262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tensorflow as tf\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "from lightgbm import LGBMRegressor\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import (accuracy_score, cohen_kappa_score,\n",
    "                             confusion_matrix, f1_score, mean_absolute_error,\n",
    "                             mean_squared_error, precision_score, recall_score,\n",
    "                             classification_report)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:30:28.989515Z",
     "iopub.status.busy": "2024-11-27T20:30:28.989171Z",
     "iopub.status.idle": "2024-11-27T20:30:29.003952Z",
     "shell.execute_reply": "2024-11-27T20:30:29.003104Z",
     "shell.execute_reply.started": "2024-11-27T20:30:28.989478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:30:29.005054Z",
     "iopub.status.busy": "2024-11-27T20:30:29.004820Z",
     "iopub.status.idle": "2024-11-27T20:30:29.015074Z",
     "shell.execute_reply": "2024-11-27T20:30:29.014279Z",
     "shell.execute_reply.started": "2024-11-27T20:30:29.005031Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed_value=2024):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/naliniramanathan/projects/ml_course/final_project/kaggle/input/cmi-piu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:30:29.016279Z",
     "iopub.status.busy": "2024-11-27T20:30:29.016017Z",
     "iopub.status.idle": "2024-11-27T20:30:29.023648Z",
     "shell.execute_reply": "2024-11-27T20:30:29.022988Z",
     "shell.execute_reply.started": "2024-11-27T20:30:29.016222Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TRAIN_CSV = f'{file_path}/train.csv'\n",
    "TEST_CSV = f'{file_path}/test.csv'\n",
    "SAMPLE_SUBMISSION_CSV = f'{file_path}/sample_submission.csv'\n",
    "SERIES_TRAIN_DIR = f'{file_path}/series_train.parquet'\n",
    "SERIES_TEST_DIR = f'{file_path}/series_test.parquet'\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "sample_submission_df = pd.read_csv(SAMPLE_SUBMISSION_CSV)\n",
    "\n",
    "# Drop all the PCIAT variables as they are not present in the test data\n",
    "for col in train_df.columns:\n",
    "    if 'PCIAT' in col:\n",
    "        train_df.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:30:29.073558Z",
     "iopub.status.busy": "2024-11-27T20:30:29.073332Z",
     "iopub.status.idle": "2024-11-27T20:30:29.078330Z",
     "shell.execute_reply": "2024-11-27T20:30:29.077425Z",
     "shell.execute_reply.started": "2024-11-27T20:30:29.073537Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to process individual time series files\n",
    "\n",
    "# Min length of the time series is 927 (hardcoded to speed this up)\n",
    "def process_time_series(file_name, directory, max_len=927):\n",
    "    df = pd.read_parquet(os.path.join(directory, file_name, 'part-0.parquet'))\n",
    "    df['id'] = file_name.split('=')[1]\n",
    "    # Adjusted this so we get the full parquet - just concatenate all of them\n",
    "    return df[:max_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:30:29.079721Z",
     "iopub.status.busy": "2024-11-27T20:30:29.079402Z",
     "iopub.status.idle": "2024-11-27T20:30:29.094876Z",
     "shell.execute_reply": "2024-11-27T20:30:29.093997Z",
     "shell.execute_reply.started": "2024-11-27T20:30:29.079687Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to load and aggregate time series data\n",
    "def load_time_series_data(directory):\n",
    "    file_names = os.listdir(directory)\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_time_series(fname, directory), file_names),\n",
    "                            total=len(file_names)))\n",
    "\n",
    "    return pd.concat(results, ignore_index=True) # This takes slightly longer but ok for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:30:29.096085Z",
     "iopub.status.busy": "2024-11-27T20:30:29.095843Z",
     "iopub.status.idle": "2024-11-27T20:31:37.731472Z",
     "shell.execute_reply": "2024-11-27T20:31:37.730517Z",
     "shell.execute_reply.started": "2024-11-27T20:30:29.096061Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [00:20<00:00, 48.26it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 34.99it/s]\n"
     ]
    }
   ],
   "source": [
    "train_series_df = load_time_series_data(SERIES_TRAIN_DIR)\n",
    "test_series_df = load_time_series_data(SERIES_TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>enmo</th>\n",
       "      <th>anglez</th>\n",
       "      <th>non-wear_flag</th>\n",
       "      <th>light</th>\n",
       "      <th>battery_voltage</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>quarter</th>\n",
       "      <th>relative_date_PCIAT</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.468869</td>\n",
       "      <td>0.412020</td>\n",
       "      <td>-0.236458</td>\n",
       "      <td>0.042506</td>\n",
       "      <td>-19.824650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.666666</td>\n",
       "      <td>4179.000000</td>\n",
       "      <td>57480000000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0d01bbf2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.662526</td>\n",
       "      <td>0.533484</td>\n",
       "      <td>0.064034</td>\n",
       "      <td>0.052847</td>\n",
       "      <td>4.300246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>4178.666504</td>\n",
       "      <td>57485000000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0d01bbf2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.611384</td>\n",
       "      <td>0.227252</td>\n",
       "      <td>-0.150882</td>\n",
       "      <td>0.060734</td>\n",
       "      <td>-16.545208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>4178.333496</td>\n",
       "      <td>57490000000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0d01bbf2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.385799</td>\n",
       "      <td>0.552782</td>\n",
       "      <td>-0.500523</td>\n",
       "      <td>0.070440</td>\n",
       "      <td>-36.452175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.799999</td>\n",
       "      <td>4178.000000</td>\n",
       "      <td>57495000000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0d01bbf2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.016133</td>\n",
       "      <td>0.031981</td>\n",
       "      <td>-0.825109</td>\n",
       "      <td>0.081058</td>\n",
       "      <td>-67.488388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4177.666504</td>\n",
       "      <td>57500000000000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0d01bbf2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923287</th>\n",
       "      <td>922</td>\n",
       "      <td>0.966741</td>\n",
       "      <td>0.028754</td>\n",
       "      <td>0.057921</td>\n",
       "      <td>0.008051</td>\n",
       "      <td>3.353292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.564102</td>\n",
       "      <td>4193.000000</td>\n",
       "      <td>46610000000000</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57de6095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923288</th>\n",
       "      <td>923</td>\n",
       "      <td>1.009239</td>\n",
       "      <td>-0.092948</td>\n",
       "      <td>-0.083056</td>\n",
       "      <td>0.043492</td>\n",
       "      <td>-4.464321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.447117</td>\n",
       "      <td>4193.000000</td>\n",
       "      <td>46615000000000</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57de6095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923289</th>\n",
       "      <td>924</td>\n",
       "      <td>0.986129</td>\n",
       "      <td>-0.272536</td>\n",
       "      <td>-0.248816</td>\n",
       "      <td>0.076904</td>\n",
       "      <td>-13.811750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.330128</td>\n",
       "      <td>4193.000000</td>\n",
       "      <td>46620000000000</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57de6095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923290</th>\n",
       "      <td>925</td>\n",
       "      <td>0.976071</td>\n",
       "      <td>-0.258840</td>\n",
       "      <td>-0.123272</td>\n",
       "      <td>0.069854</td>\n",
       "      <td>-7.051679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.213142</td>\n",
       "      <td>4193.000000</td>\n",
       "      <td>46625000000000</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57de6095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923291</th>\n",
       "      <td>926</td>\n",
       "      <td>0.959311</td>\n",
       "      <td>-0.347963</td>\n",
       "      <td>-0.206829</td>\n",
       "      <td>0.083468</td>\n",
       "      <td>-11.260011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.096153</td>\n",
       "      <td>4193.000000</td>\n",
       "      <td>46630000000000</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57de6095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>923292 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        step         X         Y         Z      enmo     anglez  \\\n",
       "0          0 -0.468869  0.412020 -0.236458  0.042506 -19.824650   \n",
       "1          1 -0.662526  0.533484  0.064034  0.052847   4.300246   \n",
       "2          2 -0.611384  0.227252 -0.150882  0.060734 -16.545208   \n",
       "3          3 -0.385799  0.552782 -0.500523  0.070440 -36.452175   \n",
       "4          4  0.016133  0.031981 -0.825109  0.081058 -67.488388   \n",
       "...      ...       ...       ...       ...       ...        ...   \n",
       "923287   922  0.966741  0.028754  0.057921  0.008051   3.353292   \n",
       "923288   923  1.009239 -0.092948 -0.083056  0.043492  -4.464321   \n",
       "923289   924  0.986129 -0.272536 -0.248816  0.076904 -13.811750   \n",
       "923290   925  0.976071 -0.258840 -0.123272  0.069854  -7.051679   \n",
       "923291   926  0.959311 -0.347963 -0.206829  0.083468 -11.260011   \n",
       "\n",
       "        non-wear_flag      light  battery_voltage     time_of_day  weekday  \\\n",
       "0                 0.0  27.666666      4179.000000  57480000000000        4   \n",
       "1                 0.0  12.666667      4178.666504  57485000000000        4   \n",
       "2                 0.0  47.000000      4178.333496  57490000000000        4   \n",
       "3                 0.0  63.799999      4178.000000  57495000000000        4   \n",
       "4                 0.0   6.000000      4177.666504  57500000000000        4   \n",
       "...               ...        ...              ...             ...      ...   \n",
       "923287            0.0  51.564102      4193.000000  46610000000000        5   \n",
       "923288            0.0  57.447117      4193.000000  46615000000000        5   \n",
       "923289            0.0  63.330128      4193.000000  46620000000000        5   \n",
       "923290            0.0  69.213142      4193.000000  46625000000000        5   \n",
       "923291            0.0  75.096153      4193.000000  46630000000000        5   \n",
       "\n",
       "        quarter  relative_date_PCIAT        id  \n",
       "0             1                 28.0  0d01bbf2  \n",
       "1             1                 28.0  0d01bbf2  \n",
       "2             1                 28.0  0d01bbf2  \n",
       "3             1                 28.0  0d01bbf2  \n",
       "4             1                 28.0  0d01bbf2  \n",
       "...         ...                  ...       ...  \n",
       "923287        4                  1.0  57de6095  \n",
       "923288        4                  1.0  57de6095  \n",
       "923289        4                  1.0  57de6095  \n",
       "923290        4                  1.0  57de6095  \n",
       "923291        4                  1.0  57de6095  \n",
       "\n",
       "[923292 rows x 14 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_series_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding of Time Series Data\n",
    "\n",
    "LTSM Autoencoder taken from: https://machinelearningmastery.com/lstm-autoencoders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:31:37.733863Z",
     "iopub.status.busy": "2024-11-27T20:31:37.733556Z",
     "iopub.status.idle": "2024-11-27T20:31:37.740454Z",
     "shell.execute_reply": "2024-11-27T20:31:37.739472Z",
     "shell.execute_reply.started": "2024-11-27T20:31:37.733819Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Encoder Class\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout, seq_len, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm_enc = nn.LSTM(input_size=input_size, num_layers=num_layers, hidden_size=hidden_size, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (last_h_state, last_c_state) = self.lstm_enc(x)\n",
    "        # Get the last hidden state\n",
    "        x_enc = last_h_state[-1, :, :]         \n",
    "        return x_enc, out\n",
    "\n",
    "\n",
    "# Decoder Class\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout, seq_len, use_act, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.seq_len = seq_len\n",
    "        self.use_act = use_act  # Parameter to control the last sigmoid activation - depends on the normalization used.\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "        self.lstm_dec = nn.LSTM(input_size=hidden_size, num_layers=num_layers, hidden_size=hidden_size, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = z.unsqueeze(1).repeat(1, self.seq_len, 1)\n",
    "        dec_out, (hidden_state, cell_state) = self.lstm_dec(z)\n",
    "        dec_out = self.fc(dec_out)\n",
    "        if self.use_act:\n",
    "            dec_out = self.act(dec_out)\n",
    "\n",
    "        return dec_out, hidden_state\n",
    "\n",
    "\n",
    "# LSTM Auto-Encoder Class\n",
    "class TimeSeriesAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_ratio, seq_len, num_layers, use_act=True):\n",
    "        super(TimeSeriesAutoencoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.encoder = Encoder(input_size=input_size, num_layers=num_layers, hidden_size=hidden_size, dropout=dropout_ratio, seq_len=seq_len)\n",
    "        self.decoder = Decoder(input_size=input_size, num_layers=num_layers, hidden_size=hidden_size, dropout=dropout_ratio, seq_len=seq_len, use_act=use_act)\n",
    "\n",
    "    def forward(self, x, return_last_h=False, return_enc_out=False):\n",
    "        x_enc, enc_out = self.encoder(x)\n",
    "        x_dec, last_h = self.decoder(x_enc)\n",
    "\n",
    "        if return_last_h:\n",
    "            return x_dec, last_h\n",
    "        elif return_enc_out:\n",
    "            return x_dec, enc_out\n",
    "        return x_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    996.0\n",
       "mean     927.0\n",
       "std        0.0\n",
       "min      927.0\n",
       "25%      927.0\n",
       "50%      927.0\n",
       "75%      927.0\n",
       "max      927.0\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# We'll pick the min value as the sequence length\n",
    "sequence_length = train_series_df['id'].value_counts().min()\n",
    "train_series_df['id'].value_counts().describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence length is hardcoded at 927\n",
    "def prepare_data(df):\n",
    "    # Convert the dataframe to a numpy array\n",
    "    ids = df['id'].unique()\n",
    "    num_samples = len(ids)\n",
    "    num_features = len(df.columns) - 2\n",
    "\n",
    "    data = np.array(df.drop(['id', 'step'], axis = 1).values)\n",
    "    # Convert the numpy array to a PyTorch tensor\n",
    "    reshaped_data = data.reshape(num_samples, 927, num_features)\n",
    "\n",
    "    data_tensor = torch.tensor(reshaped_data, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    return data_tensor, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor, ids = prepare_data(train_series_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([996, 927, 12])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:31:37.741702Z",
     "iopub.status.busy": "2024-11-27T20:31:37.741472Z",
     "iopub.status.idle": "2024-11-27T20:31:37.754091Z",
     "shell.execute_reply": "2024-11-27T20:31:37.753296Z",
     "shell.execute_reply.started": "2024-11-27T20:31:37.741674Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_encoded_features(df, lr=0.01, hidden_size=12, num_layers=2, device = 'mps', dropout_ratio=0.2, epochs=100, batch_size=32, sequence_length = 927):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df.drop('id', axis=1))\n",
    "\n",
    "    # Reshape to df for easier column handling and id tracking\n",
    "    incl_columns = list(df.columns)\n",
    "    incl_columns.remove('id')\n",
    "    print(incl_columns)\n",
    "    scaled_data_df = pd.DataFrame(scaled_data, columns=incl_columns)\n",
    "    scaled_data_df['id'] = df['id']\n",
    "    \n",
    "    tensor_data, ids = prepare_data(scaled_data_df)\n",
    "    input_size = tensor_data.shape[2]\n",
    "\n",
    "    autoencoder = TimeSeriesAutoencoder(input_size, hidden_size, dropout_ratio, sequence_length, num_layers=num_layers)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=lr)\n",
    "\n",
    "    device = torch.device('mps')\n",
    "    autoencoder = autoencoder.to(device)\n",
    "\n",
    "    dataset = TensorDataset(tensor_data)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Step 5: Training Loop\n",
    "    autoencoder.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in data_loader:\n",
    "            batch_data = batch[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = autoencoder(batch_data)\n",
    "            loss = criterion(outputs, batch_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(data_loader)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Avg Loss: {avg_epoch_loss:.4f}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tensor_data = tensor_data.to(device)\n",
    "        encoded_data, vals = autoencoder.encoder(tensor_data)  # Get encoded features\n",
    "\n",
    "    encoded_data = encoded_data.cpu().numpy()\n",
    "\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n",
    "    encoded_df['id'] = ids\n",
    "    return encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['step', 'X', 'Y', 'Z', 'enmo', 'anglez', 'non-wear_flag', 'light', 'battery_voltage', 'time_of_day', 'weekday', 'quarter', 'relative_date_PCIAT']\n",
      "Epoch [10/100], Avg Loss: 0.8430\n",
      "Epoch [20/100], Avg Loss: 0.8066\n",
      "Epoch [30/100], Avg Loss: 0.8017\n",
      "Epoch [40/100], Avg Loss: 0.7966\n",
      "Epoch [50/100], Avg Loss: 0.7732\n",
      "Epoch [60/100], Avg Loss: 0.7590\n",
      "Epoch [70/100], Avg Loss: 0.7601\n",
      "Epoch [80/100], Avg Loss: 0.7491\n",
      "Epoch [90/100], Avg Loss: 0.7464\n",
      "Epoch [100/100], Avg Loss: 0.7579\n"
     ]
    }
   ],
   "source": [
    "train_encoded = get_encoded_features(train_series_df, hidden_size=16, num_layers=1, dropout_ratio=0.1)\n",
    "# test_encoded = get_encoded_features(test_series_df) # Eventually use the same model to encode the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(train_encoded, on='id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation of Missing Numerical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:31:47.339825Z",
     "iopub.status.busy": "2024-11-27T20:31:47.339540Z",
     "iopub.status.idle": "2024-11-27T20:31:47.346242Z",
     "shell.execute_reply": "2024-11-27T20:31:47.345349Z",
     "shell.execute_reply.started": "2024-11-27T20:31:47.339800Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Imputing missing values using KNN imputer\n",
    "def impute_missing_values(df):\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    numeric_columns = df.select_dtypes(include=['float64','float32','int64']).columns # We treat the time series data as a median for now but others could be used\n",
    "    imputed_array = imputer.fit_transform(df[numeric_columns])\n",
    "    imputed_df = pd.DataFrame(imputed_array, columns=numeric_columns)\n",
    "    for col in df.columns:\n",
    "        if col not in numeric_columns:\n",
    "            imputed_df[col] = df[col]\n",
    "    return imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic_Demos-Age           0\n",
      "Basic_Demos-Sex           0\n",
      "CGAS-CGAS_Score           0\n",
      "Physical-BMI              0\n",
      "Physical-Height           0\n",
      "                       ... \n",
      "BIA-Season             1815\n",
      "PAQ_A-Season           3485\n",
      "PAQ_C-Season           2239\n",
      "SDS-Season             1342\n",
      "PreInt_EduHx-Season     420\n",
      "Length: 76, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df.replace([np.inf, -np.inf], np.nan, inplace=True) # Debug why we have inf values\n",
    "\n",
    "train_df = impute_missing_values(train_df)\n",
    "train_df['sii'] = train_df['sii'].round().astype(int)\n",
    "\n",
    "# Imputation is needed in test set for some cases but not others to revisit\n",
    "\n",
    "print(train_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', 'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', 'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = list(train_df.select_dtypes(include=['object']).columns) #sii (outcome var) is categorical but we are encoding that differently\n",
    "categorical_cols.remove('id')\n",
    "print(categorical_cols)\n",
    "\n",
    "def preprocess_categorical(df):\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].fillna('Missing').astype('category')\n",
    "    return df\n",
    "\n",
    "train_df = preprocess_categorical(train_df)\n",
    "test_df = preprocess_categorical(test_df)\n",
    "\n",
    "\n",
    "train_df = pd.get_dummies(train_df, columns = categorical_cols, drop_first=True, dtype='int')\n",
    "test_df = pd.get_dummies(test_df, columns = categorical_cols, drop_first=True, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:31:54.047091Z",
     "iopub.status.busy": "2024-11-27T20:31:54.046800Z",
     "iopub.status.idle": "2024-11-27T20:31:54.053460Z",
     "shell.execute_reply": "2024-11-27T20:31:54.052581Z",
     "shell.execute_reply.started": "2024-11-27T20:31:54.047062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Feature engineering\n",
    "# def add_engineered_features(df):\n",
    "#     df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
    "#     df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
    "#     df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
    "#     df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
    "#     df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n",
    "#     df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n",
    "#     df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n",
    "#     df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
    "#     df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n",
    "#     df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
    "#     df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n",
    "#     df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n",
    "#     df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
    "#     df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n",
    "#     df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:31:54.054834Z",
     "iopub.status.busy": "2024-11-27T20:31:54.054560Z",
     "iopub.status.idle": "2024-11-27T20:31:54.077742Z",
     "shell.execute_reply": "2024-11-27T20:31:54.076869Z",
     "shell.execute_reply.started": "2024-11-27T20:31:54.054809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_df = add_engineered_features(train_df)\n",
    "# test_df = add_engineered_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:31:54.078875Z",
     "iopub.status.busy": "2024-11-27T20:31:54.078645Z",
     "iopub.status.idle": "2024-11-27T20:31:54.088982Z",
     "shell.execute_reply": "2024-11-27T20:31:54.088223Z",
     "shell.execute_reply.started": "2024-11-27T20:31:54.078852Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df.drop('id', axis=1)\n",
    "test_df = test_df.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:31:54.092957Z",
     "iopub.status.busy": "2024-11-27T20:31:54.092741Z",
     "iopub.status.idle": "2024-11-27T20:31:54.098130Z",
     "shell.execute_reply": "2024-11-27T20:31:54.097298Z",
     "shell.execute_reply.started": "2024-11-27T20:31:54.092936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# encoded_feature_cols = [col for col in train_encoded.columns if col != 'id']\n",
    "# selected_features = [\n",
    "#     'Basic_Demos-Age', 'Basic_Demos-Sex', 'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "#     'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "#     'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "#     'Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time_Mins',\n",
    "#     'Fitness_Endurance-Time_Sec', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone',\n",
    "#     'FGC-FGC_GSND', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone',\n",
    "#     'FGC-FGC_PU', 'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone',\n",
    "#     'FGC-FGC_SRR', 'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
    "#     'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "#     'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "#     'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "#     'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "#     'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total', 'PAQ_C-PAQ_C_Total',\n",
    "#     'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T',\n",
    "#     'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age', 'Internet_Hours_Age',\n",
    "#     'BMI_Internet_Hours', 'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW',\n",
    "#     'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight', 'SMM_Height',\n",
    "#     'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW'\n",
    "# ] + encoded_feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:31:54.099518Z",
     "iopub.status.busy": "2024-11-27T20:31:54.099172Z",
     "iopub.status.idle": "2024-11-27T20:31:54.121715Z",
     "shell.execute_reply": "2024-11-27T20:31:54.120888Z",
     "shell.execute_reply.started": "2024-11-27T20:31:54.099481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_df = train_df[selected_features + ['sii']].dropna(subset=['sii'])\n",
    "# test_df = test_df[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - LGBM, XGBoost, CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:31:54.130492Z",
     "iopub.status.busy": "2024-11-27T20:31:54.130142Z",
     "iopub.status.idle": "2024-11-27T20:31:54.138591Z",
     "shell.execute_reply": "2024-11-27T20:31:54.137783Z",
     "shell.execute_reply.started": "2024-11-27T20:31:54.130451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def quadratic_weighted_kappa(y_actual, y_predicted):\n",
    "    return cohen_kappa_score(y_actual, y_predicted, weights='quadratic')\n",
    "\n",
    "def apply_thresholds(predictions, thresholds):\n",
    "    return np.where(predictions < thresholds[0], 0,\n",
    "                    np.where(predictions < thresholds[1], 1,\n",
    "                             np.where(predictions < thresholds[2], 2, 3)))\n",
    "\n",
    "def optimize_thresholds(y_true, predictions):\n",
    "    def loss_func(thresh):\n",
    "        return -quadratic_weighted_kappa(y_true, apply_thresholds(predictions, thresh))\n",
    "    initial_thresholds = [0.5, 1.5, 2.5]\n",
    "    result = minimize(loss_func, initial_thresholds, method='Nelder-Mead')\n",
    "    return result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:31:54.139858Z",
     "iopub.status.busy": "2024-11-27T20:31:54.139618Z",
     "iopub.status.idle": "2024-11-27T20:31:54.155262Z",
     "shell.execute_reply": "2024-11-27T20:31:54.154575Z",
     "shell.execute_reply.started": "2024-11-27T20:31:54.139835Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(model):\n",
    "    X = train_df.drop('sii', axis=1)\n",
    "    y = train_df['sii'].astype(int)\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    oof_predictions = np.zeros(len(y))\n",
    "    # test_predictions = np.zeros(len(test_features))\n",
    "    train_kappas = []\n",
    "    val_kappas = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "        print(f\"Training fold {fold + 1}\")\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        print(np.any(np.isnan(X_train)), np.any(np.isnan(y_train)))  # Should be False\n",
    "        print(np.any(np.isinf(X_train)), np.any(np.isinf(y_train)))  # Should be False\n",
    "\n",
    "        print(np.any(np.isnan(X_val)), np.any(np.isnan(y_val)))  # Should be False\n",
    "        print(np.any(np.isinf(X_val)), np.any(np.isinf(y_val)))  # Should be False\n",
    "\n",
    "        cloned_model = clone(model)\n",
    "        cloned_model.fit(X_train, y_train)\n",
    "        \n",
    "        y_train_pred = cloned_model.predict(X_train).round().astype(int)\n",
    "        y_val_pred = cloned_model.predict(X_val)\n",
    "\n",
    "        oof_predictions[val_idx] = y_val_pred\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred)\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred.round().astype(int))\n",
    "\n",
    "        train_kappas.append(train_kappa)\n",
    "        val_kappas.append(val_kappa)\n",
    "\n",
    "        # test_predictions += cloned_model.predict(test_features) / kf.n_splits\n",
    "\n",
    "        print(f\"Fold {fold + 1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Average Train QWK: {np.mean(train_kappas):.4f}\")\n",
    "    print(f\"Average Validation QWK: {np.mean(val_kappas):.4f}\")\n",
    "\n",
    "    optimal_thresholds = optimize_thresholds(y, oof_predictions)\n",
    "    print(f\"Optimized Thresholds: {optimal_thresholds}\")\n",
    "\n",
    "    final_oof_predictions = apply_thresholds(oof_predictions, optimal_thresholds)\n",
    "    # final_test_predictions = apply_thresholds(test_predictions, optimal_thresholds)\n",
    "\n",
    "    final_kappa = quadratic_weighted_kappa(y, final_oof_predictions)\n",
    "    print(f\"Final Optimized QWK: {Fore.CYAN}{Style.BRIGHT}{final_kappa:.4f}{Style.RESET_ALL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:31:54.156401Z",
     "iopub.status.busy": "2024-11-27T20:31:54.156148Z",
     "iopub.status.idle": "2024-11-27T20:31:54.172735Z",
     "shell.execute_reply": "2024-11-27T20:31:54.171901Z",
     "shell.execute_reply.started": "2024-11-27T20:31:54.156377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lightgbm_params = {\n",
    "    'learning_rate': 0.046,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 478,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.784,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,\n",
    "    'lambda_l2': 0.01,\n",
    "    'n_estimators': 300,\n",
    "    'random_state': 42,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "xgboost_params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,\n",
    "    'reg_lambda': 5,\n",
    "    'random_state': 42,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "# xgboost_params = {\n",
    "#     'learning_rate': 0.05,\n",
    "#     'max_depth': 6,\n",
    "#     'n_estimators': 200,\n",
    "#     'subsample': 0.8,\n",
    "#     'colsample_bytree': 0.8,\n",
    "#     'reg_alpha': 1,\n",
    "#     'reg_lambda': 5,\n",
    "#     'random_state': 42,\n",
    "#     'tree_method': 'gpu_hist',\n",
    "#     'verbosity': 0\n",
    "# }\n",
    "\n",
    "catboost_params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': 42,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10\n",
    "}\n",
    "\n",
    "\n",
    "# catboost_params = {\n",
    "#     'learning_rate': 0.05,\n",
    "#     'depth': 6,\n",
    "#     'iterations': 200,\n",
    "#     'random_seed': 42,\n",
    "#     'verbose': 0,\n",
    "#     'l2_leaf_reg': 10,\n",
    "#     'task_type': 'GPU'\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:31:54.173797Z",
     "iopub.status.busy": "2024-11-27T20:31:54.173562Z",
     "iopub.status.idle": "2024-11-27T20:31:54.185192Z",
     "shell.execute_reply": "2024-11-27T20:31:54.184507Z",
     "shell.execute_reply.started": "2024-11-27T20:31:54.173774Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lgb_model = LGBMRegressor(**lightgbm_params)\n",
    "xgb_model = XGBRegressor(**xgboost_params)\n",
    "cat_model = CatBoostRegressor(**catboost_params)\n",
    "ensemble_model = VotingRegressor(\n",
    "    estimators=[('lgb', lgb_model), ('xgb', xgb_model), ('cat', cat_model)],\n",
    "    weights=[4.0, 4.0, 5.0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:31:54.197977Z",
     "iopub.status.busy": "2024-11-27T20:31:54.197736Z",
     "iopub.status.idle": "2024-11-27T20:32:09.386372Z",
     "shell.execute_reply": "2024-11-27T20:32:09.385657Z",
     "shell.execute_reply.started": "2024-11-27T20:31:54.197954Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train QWK: 0.7878\n",
      "Average Validation QWK: 0.4548\n",
      "Optimized Thresholds: [0.56900213 1.05590682 2.83650092]\n",
      "Final Optimized QWK: \u001b[36m\u001b[1m0.4971\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(ensemble_model)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6179764,
     "sourceId": 10033356,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ml_final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
