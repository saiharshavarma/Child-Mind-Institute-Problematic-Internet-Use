{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLProject\n",
    "\n",
    "Group 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from scipy.optimize import minimize\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "N_SPLITS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    \n",
    "    stats, indexes = zip(*results)\n",
    "    \n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    season_cols = [col for col in df.columns if 'Season' in col]\n",
    "    df = df.drop(season_cols, axis=1) \n",
    "    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
    "    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
    "    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
    "    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
    "    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n",
    "    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n",
    "    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n",
    "    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
    "    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n",
    "    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
    "    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n",
    "    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n",
    "    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
    "    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n",
    "    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [00:23<00:00, 42.56it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 26.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load train and test datasets\n",
    "train_data = pd.read_csv('./data/train.csv')\n",
    "test_data = pd.read_csv('./data/test.csv')\n",
    "\n",
    "# Load time series data\n",
    "train_time_series = load_time_series(\"./data/series_train.parquet\")\n",
    "test_time_series = load_time_series(\"./data/series_test.parquet\")\n",
    "\n",
    "# Merge time series data with main datasets\n",
    "train_data = pd.merge(train_data, train_time_series, how=\"left\", on='id')\n",
    "test_data = pd.merge(test_data, test_time_series, how=\"left\", on='id')\n",
    "\n",
    "# Impute missing values\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "numeric_cols = train_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "train_data[numeric_cols] = imputer.fit_transform(train_data[numeric_cols])\n",
    "\n",
    "# Feature engineering\n",
    "train_data = feature_engineering(train_data)\n",
    "test_data = feature_engineering(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AutoEncoder for dimensionality reduction\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*3, encoding_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*2, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*2, input_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*3, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metric: Quadratic Weighted Kappa\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "# Threshold rounding function\n",
    "def threshold_rounder(predictions, thresholds):\n",
    "    return np.where(predictions < thresholds[0], 0,\n",
    "                    np.where(predictions < thresholds[1], 1,\n",
    "                             np.where(predictions < thresholds[2], 2, 3)))\n",
    "\n",
    "# Evaluate predictions with thresholds\n",
    "def evaluate_predictions(thresholds, y_true, predictions):\n",
    "    rounded_predictions = threshold_rounder(predictions, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate model\n",
    "def train_and_evaluate_model(model_class, X_train, y_train, X_test):\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    oof_predictions = np.zeros(len(y_train))\n",
    "    test_predictions = np.zeros(len(X_test))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        print(f\"Training fold {fold}\")\n",
    "        \n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = model_class(random_state=SEED)\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        oof_predictions[val_idx] = model.predict(X_val_fold)\n",
    "        test_predictions += model.predict(X_test) / N_SPLITS\n",
    "    \n",
    "    # Optimize thresholds\n",
    "    initial_thresholds = [0.5, 1.5, 2.5]\n",
    "    optimized_thresholds = minimize(evaluate_predictions, initial_thresholds, \n",
    "                                    args=(y_train, oof_predictions), \n",
    "                                    method='nelder-mead').x\n",
    "    \n",
    "    # Calculate final score\n",
    "    final_score = -evaluate_predictions(optimized_thresholds, y_train, oof_predictions)\n",
    "    print(f\"Final Quadratic Weighted Kappa score: {final_score}\")\n",
    "    \n",
    "    return test_predictions, optimized_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_classes(y):\n",
    "    return np.digitize(y, bins=[0.5, 1.5, 2.5]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m model_predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_class \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m---> 12\u001b[0m     predictions, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     model_predictions\u001b[38;5;241m.\u001b[39mappend(predictions)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Ensemble predictions\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(model_class, X_train, y_train, X_test)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m X_train_fold, X_val_fold \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39miloc[train_idx], X_train\u001b[38;5;241m.\u001b[39miloc[val_idx]\n\u001b[0;32m---> 12\u001b[0m y_train_fold, y_val_fold \u001b[38;5;241m=\u001b[39m \u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m[train_idx], y_train\u001b[38;5;241m.\u001b[39miloc[val_idx]\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class(random_state\u001b[38;5;241m=\u001b[39mSEED)\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train_fold, y_train_fold)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "features = [col for col in train_data.columns if col not in ['id', 'sii']]\n",
    "X_train = train_data[features]\n",
    "y_train = convert_to_classes(train_data['sii'])\n",
    "available_features = [col for col in features if col in test_data.columns]\n",
    "X_test = test_data[available_features]\n",
    "\n",
    "# Train and evaluate models\n",
    "models = [LGBMRegressor, XGBRegressor]\n",
    "model_predictions = []\n",
    "\n",
    "for model_class in models:\n",
    "    predictions, thresholds = train_and_evaluate_model(model_class, X_train, y_train, X_test)\n",
    "    model_predictions.append(predictions)\n",
    "\n",
    "# Ensemble predictions\n",
    "final_predictions = np.mean(model_predictions, axis=0)\n",
    "\n",
    "print(final_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
